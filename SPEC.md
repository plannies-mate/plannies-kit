# Plannies Mate Specification

## Purpose
A Ruby command-line tool that analyzes PlanningAlerts scrapers to extract unique identifiers and terms. These help the "Cricky, what's that?" frontend tool identify which scraper to use for different council websites.

## Core Requirements

### Word Extraction Rules
1. All text is converted to lowercase first
2. Words are alphanumeric sequences matching `/[a-z0-9]+/`
3. Words are separated by any non-alphanumeric characters `/[^a-z0-9]+/`
4. No special handling of camelCase, snake_case, or kebab-case
5. Words must be 3+ characters long
6. Words are filtered against:
   - Common word list (COMMON_WORDS)
   - English dictionary via aspell

### Example Word Extraction
```
Input URL: https://www.yarracity.vic.gov.au/planning-application-search
Extracted words: ["yarracitygovau", "planningapplicationsearch"]

Input URL: https://www.planning.act.gov.au/development_applications
Extracted words: ["planningactgovau", "developmentapplications"]

Input: CurrentAdvertisedPlans
Extracted word: "currentadvertisedplans"
```

## Output Format

### scraper_analysis.js
```javascript
// Generated by Plannies Mate at 2025-01-29T11:40:09Z
// Including 31 active scrapers
// Excluded 4 trivial scrapers
// Excluded 2 broken scrapers
// Excluded 3 repos without scraper files

export const scraperData = {
   'yarra': {
      description: "Yarra City Development Applications",
      words: ["yarracitygovau", "planningapplicationsearch"]
   },
   'act': {
      description: "ACT Planning Portal Scraper",
      words: ["planningactgovau", "developmentapplications",
         "currentadvertisedplans"]
   }
   // ... more scrapers
};
```

### debug_analysis.json
```json
{
  "metadata": {
    "generated_at": "2025-01-29T11:40:09Z",
    "repos_analyzed": 40,
    "trivial_scrapers_skipped": 4,
    "broken_scrapers_found": 2,
    "no_scraper_file": 3
  },
  "repos": {
    "yarra": {
      "name": "yarra",
      "description": "Yarra City Development Applications",
      "status": "active",
      "urls": [
        "https://www.yarracity.vic.gov.au/planning-application-search"
      ],
      "words": ["yarracitygovau", "planningapplicationsearch"],
      "main_line_count": 47,
      "total_line_count": 47
    }
    // ... more repos
  }
}
```

## Scripts and Their Responsibilities

### download.rb
- Downloads non-archived PlanningAlerts scraper repositories
- Supports optional LIMIT parameter for testing
- Stores repository descriptions in descriptions.json
- Skips existing repositories

### cleanup.rb
- Removes test files, binary files, and test directories
- Removes .git directories and unnecessary project files
- Handles common binary file extensions

### analyze.rb
- Analyzes code files for unique terms
- Extracts and processes URLs
- Filters words using aspell and COMMON_WORDS
- Outputs scraper_analysis.js and debug_analysis.json

## Process Management
- aspell process must be properly initialized and cleaned up
- Handle aspell errors gracefully with recovery attempts
- Use at_exit hook to ensure process cleanup

## Error Handling
- Scripts validate arguments and fail fast
- Handle GitHub API errors gracefully
- Report progress clearly but concisely
- Output clear error messages with potential solutions

## Dependencies
- Ruby 3.3 or newer
- aspell
- Git

## Project Files
```
.
├── Gemfile
├── Gemfile.lock
├── lib/
│   ├── analyze.rb  # Main analysis code
│   ├── cleanup.rb  # Repo cleanup code
│   └── download.rb # GitHub download code
└── script/
    ├── process     # Runs all scripts
    └── clobber     # Cleans repos dir
```

## Code Structure & Style
- Ruby code in `lib/` directory, executed from project root
   - script/process runs all the lib/*.rb files
- Scripts require explicit repo directory parameter
- Scripts handle one focused task each
- Variable names reflect planning/scraping domain
- Follow standard Ruby style practices

## Command Line Interface
- Scripts validate arguments and fail fast with clear errors
- Progress reporting is clear but concise
- Optional LIMIT parameter to process fewer repos for testing

## Performance & Security
- Skip re-downloading existing repos
- Cleanup removes binary files so process just processes the files that are left
- Handle GitHub API rate limiting and pagination
   - do a git clone on the page you got before requesting the next page
- Validate repository names and file paths
- Don't execute content from repos

## Development Practices
- Use RuboCop for code style (when/if needed)
- Document dependencies in Gemfile
- Keep commits focused and well-described
- Ignore appropriate files (repos/, temp files)
