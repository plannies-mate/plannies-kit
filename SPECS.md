# Plannies Mate Specification

## Purpose

A Ruby command-line tool that analyzes PlanningAlerts scrapers to extract unique identifiers and terms. These help the "
Cricky, what's that?" frontend tool identify which scraper to use for different council websites.

## Project Focus

Making scraper identification easier and more reliable for the "Cricky, what's that?" frontend tool by extracting and
analyzing unique identifiers and terms from PlanningAlerts scrapers.

## Core Requirements

### Word Extraction Rules

1. We are extracting urls from the files remaining after cleanup
    - Discard the scheme and hostname from the URL
    - Keep the path and query
1. All text is converted to lowercase first
2. Words are alphanumeric sequences matching `/[a-z0-9]+/`
3. Words are separated by any non-alphanumeric characters `/[^a-z0-9]+/`
4. This implies No special handling of
    - camelCase (becomes one word),
    - snake_case or kebab-case (which are split up into multiple words)
5. Words must be 3+ characters long
6. Words are filtered against:
    - Common word list (COMMON_WORDS) - will be updated as relevant keywords become known
    - English dictionary via aspell

### Example Word Extraction

```
Input URL: https://www.yarracity.vic.gov.au/MyPlanning-application-xsearch
Extracted words: ["myplanning", "xearch"]

Input URL: https://www.planning.act.gov.au/development_applications?fromDaste=20251012
Extracted words: ["fromdate"]
```

## Output Format

### scraper_analysis.js

This format must not be changed without agreement as it affects the other program that relies on this data.
In particular both have to have the same exact rules of how words are formed, and that the words here are only from the
code, not the repo name or description.

```javascript
// Generated by Plannies Mate at 2025-01-29T11:40:09Z
// Including 31 active scrapers
// Excluded 4 trivial scrapers
// Excluded 2 broken scrapers
// Excluded 3 repos without scraper files

// the date and time the data was last checked with github and downloaded
export const dataGeneratedAt = "2025-01-29 23:33:13.765103714 +1100";

export const scraperData = {
    'yarra': {
        description: "Yarra City Development Applications",
        words: ["myplanning", "xearch"]
    },
    'act': {
        description: "ACT Planning Portal Scraper",
        words: ["fromdate"]
    }
    // ... more scrapers
};
```

### debug_analysis.json

This format is flexible - it needs to include all relevant information useful for debugging but casn change as
processing needs change.

```json
{
  "metadata": {
    "generated_at": "2025-01-29T11:40:09Z",
    "repos_analyzed": 40,
    "trivial_scrapers_skipped": 4,
    "broken_scrapers_found": 2,
    "no_scraper_file": 3
  },
  "repos": {
    "yarra": {
      "name": "yarra",
      "description": "Yarra City Development Applications",
      "status": "active",
      "urls": [
        "https://www.yarracity.vic.gov.au/MyPlanning-application-xsearch"
      ],
      "words": ["myplanning", "xearch"],
      "main_line_count": 47,
      "total_line_count": 47
    }
    // ... more repos
  }
}
```

## Scripts and Their Responsibilities

### download.rb

- Downloads list of github repositories
  [non-archived PlanningAlerts](https://github.com/orgs/planningalerts-scrapers/repositories.json?q=archived%3Afalse)
  scraper repositories
- Supports optional LIMIT parameter for testing
- Stores repository descriptions in descriptions.json
- Skips existing repositories

### cleanup.rb

- Removes test files, binary files, and test directories
- Removes .git directories and unnecessary project files
- Handles common binary file extensions

### analyze.rb

- Analyzes code files for unique terms
- Extracts and processes URLs
- Filters words using aspell and COMMON_WORDS
- Outputs scraper_analysis.js and debug_analysis.json

## Process Management

- aspell process must be properly initialized and cleaned up
- Handle aspell errors gracefully with recovery attempts
- Use at_exit hook to ensure process cleanup

## Error Handling

- Scripts validate arguments and fail fast
- Handle GitHub API errors gracefully
- Report progress clearly but concisely
- Output clear error messages with potential solutions

## Dependencies

- Ruby 3.3 or newer
- aspell
- Git

## Project Files

```
.
├── Gemfile
├── Gemfile.lock
├── lib/
│   ├── analyze.rb  # Main analysis code
│   ├── cleanup.rb  # Repo cleanup code
│   └── download.rb # GitHub download code
└── script/
    ├── process     # Runs all scripts
    └── clobber     # Cleans repos dir
```

## Code Structure & Style

- Ruby code in `lib/` directory, executed from project root
    - script/process runs all the lib/*.rb files
- Scripts require explicit repo directory parameter
- Scripts handle one focused task each
- Variable names reflect planning/scraping domain
- Follow standard Ruby style practices

## Command Line Interface

- Scripts validate arguments and fail fast with clear errors
- Progress reporting is clear but concise
- Optional LIMIT parameter to process fewer repos for testing

## Performance & Security

- Skip re-downloading existing repos
- Cleanup removes binary files so process just processes the files that are left
- Handle GitHub API rate limiting and pagination
    - do a git clone on the page you got before requesting the next page
- Validate repository names and file paths
- Don't execute content from repos

## Development Practices

- Use RuboCop for code style (when/if needed)
- Document dependencies in Gemfile
- Keep commits focused and well-described
- Ignore appropriate files (repos/, temp files)
- Test with small LIMIT values during development
- Add new common words to COMMON_WORDS when found
- Document any aspell configuration changes needed

## Implementation Requirements

- Double-check word extraction rules before making changes
- No assumptions about URL formats
- Process paths and query parameters separately
- Use consistent word extraction across all sources
- Handle domain variants (www., .gov.au vs .gov)
- Extract words from repository descriptions when available
- Skip archived repositories during download
- Handle GitHub API pagination correctly

## Common Words Management

- Common words list is maintained in COMMON_WORDS constant
- Words must be lowercase
- Words should be commonly found in planning/government contexts
- Review and update list periodically based on results

## Process Flow

1. Download repositories:
    - Skip if repos exist and recent
    - Handle rate limits and pagination
    - Store descriptions.json
2. Clean repositories:
    - Remove test files and binaries
    - Keep main scraper files
3. Analyze code:
    - Find and read scraper files
    - Extract and normalize words
    - Filter through aspell
    - Generate output files

## Quality Checks

- Verify word extraction matches rules exactly
- Check aspell process cleanup
- Validate output file formats
- Confirm repository counts match
- Ensure proper error handling
