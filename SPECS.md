# Plannies Mate Specifications

Plannies Mate is a Ruby command-line tool that analyzes PlanningAlerts scrapers to extract unique identifiers and terms.

This helps the "Cricky, what's that?" frontend tool identify which scraper to use for different council websites,
minimizing the need to check repositories added after this analysis.

Related Documentation:
- IMPLEMENTATION.md - Detailed implementation guidance and algorithms
- GUIDELINES.md - Guidelines for AI assistants and developers

Note: README.md contains installation and usage instructions which are not relevant for AIs.

## REQUIREMENTS

* Creating `log/scraper_analysis.js`
* Validate internal and output files against these specifications
* the different stages of processing must be separate so they can be run and validated independently against the
  relevant specifications

### scraper_analysis.js format

This file is the main requirement, as the file is copied to the "Cricky, what's that?" project.

This file MUST contain:

* `export const scraperDateTime` - with a string parsable by javascript as a date time.

* `export const scraperData` - a hash of repo name from Github to the following details
    * description - the description of the repo from Github
    * words - an array of non dictionary words that are significant in scoring the usefulness of this repo in parsing
      the page in question - see word extraction rules below

```javascript
// Generated by Plannies Mate at 2025-01-29 11:40:09 +11:00
// for 31 active scrapers

// the date and time the github repo list was last checked and downloaded
export const scraperDateTime = "2025-01-29 23:33:13.765103714 +1100";

export const scraperData = {
    'yarra': {
        description: "Yarra City Development Applications",
        words: ["myplanning", "xearch"]
    },
    'act': {
        description: "ACT Planning Portal Scraper",
        words: ["fromdate"]
    }
    // ... more scrapers
};
```

### Word Extraction Rules

Both this and the "Cricky Whats That?" projects MUST follow this algorithm so search words match!

Process files by:
1. lowercase all text
2. extract all urls in the form: `/https?:\/\/[^'"]+/`
3. remove the leading scheme and hostname using: `/https?:\/\/[^\/]*/`
4. extract all sequences of `/([a-z0-9]+)/` which are called words for this project
   - Note, this means there is No special handling
    - camelCase (becomes one word),
    - snake_case or kebab-case (they are split up into multiple words)
5. ignore words that are:
   - 1 or 2 characters long
   - appear in the COMMON_WORDS list
   - are dictionary words as filtered out by `aspell list`

#### Example Word Extraction

```
Input URL: https://www.yarracity.vic.gov.au/MyPlanning-application-xsearch
Extracted words: ["myplanning", "xearch"]

Input URL: https://www.planning.act.gov.au/development_applications?fromDaste=20251012
Extracted words: ["fromdate"]
```

## ARCHITECTURE

### File layout and structure

```
.
├── Gemfile
├── Gemfile.lock
├── GUIDELINES.md   # Guidelines and conventions focused on AIs but useful for humans as well
├── README.md       # How to install, use and contribute to this project
├── SPECS.md        # This file containg specifications
├── lib/            # ruby scripts for each stage of processing
│   └── validate/   # ruby Validation scripts to validate output of processing scripts
├── log/            # Log files output from script/process and lib/analysis
└── script/         # bash scripts run by user
    ├── process     # Runs all scripts
    └── clobber     # Cleans repos dir
```

## Dependencies

Fail fast with message if missing where required for processing

- Ruby 3.3 or newer
- aspell (specifically `aspell list`)
- Git
- gems listed in `Gemfile`
- Github API for repository information

### Data formats

#### github repo format

Request non-archived PlanningAlerts repo list from Github using
[standard call](https://github.com/orgs/planningalerts-scrapers/repositories.json?q=archived%3Afalse)

Note
- a "pageCount" is returned, append `&page=N` to the url above for 2nd and subsequent pages
- normally a page of 30 records is returned, but just accept what is given and work off page
    numbers

An example of two records is as follows:

```json
{
  "payload": {
    "pageCount": 2,
    "repositories": [
      {
        "type": "Public",
        "name": "bawbaw",
        "owner": "planningalerts-scrapers",
        "isFork": true,
        "description": "Baw Baw Shire Council Planning Applications",
        "allTopics": [],
        "primaryLanguage": {
          "name": "Ruby",
          "color": "#701516"
        },
        "pullRequestCount": 0,
        "issueCount": 0,
        "starsCount": 0,
        "forksCount": 2,
        "license": null,
        "participation": null,
        "lastUpdated": {
          "hasBeenPushedTo": true,
          "timestamp": "2019-05-18T00:38:15.692Z"
        }
      },
      {
        "type": "Public",
        "name": "townsville",
        "owner": "planningalerts-scrapers",
        "isFork": false,
        "description": "Townsville City Council Development Applications",
        "allTopics": [],
        "primaryLanguage": {
          "name": "PHP",
          "color": "#4F5D95"
        },
        "pullRequestCount": 0,
        "issueCount": 0,
        "starsCount": 1,
        "forksCount": 7,
        "license": null,
        "participation": null,
        "lastUpdated": {
          "hasBeenPushedTo": true,
          "timestamp": "2019-05-07T23:08:34.542Z"
        }
      }
    ]
  }
} 
```

