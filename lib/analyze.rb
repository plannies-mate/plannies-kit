#!/usr/bin/env ruby

require 'json'
require 'uri'
require 'open3'
require 'fileutils'
require 'time'

class ScraperAnalyzer
  SPELL_CMD = 'aspell list'  # Assumes aspell is installed

  def initialize(repo_dir)
    @repo_dir = repo_dir
    abort "Directory #{repo_dir} does not exist!" unless Dir.exist?(repo_dir)
    load_descriptions
  end

  def analyze
    @results = {
      metadata: {
        generated_at: Time.now.utc.iso8601,
        repos_analyzed: 0
      },
      repos: {}  # Will hold repo data
    }

    Dir.glob(File.join(@repo_dir, '*')).sort.each do |repo_path|
      next unless File.directory?(repo_path)
      next if File.basename(repo_path) == '.git'  # Skip .git directory
      analyze_repo(repo_path)
      @results[:metadata][:repos_analyzed] += 1
    end

    output_results
  end

  private

  def load_descriptions
    desc_file = File.join(@repo_dir, 'descriptions.json')
    if File.exist?(desc_file)
      @descriptions = JSON.parse(File.read(desc_file))
    else
      puts "Warning: descriptions.json not found in #{@repo_dir}"
      @descriptions = {}
    end
  end

  def analyze_repo(repo_path)
    repo_name = File.basename(repo_path)
    puts "\nAnalyzing #{repo_name}..."

    urls = Set.new
    words = Set.new

    Dir.chdir(repo_path) do
      code_files = Dir.glob("**/*")  # All files that weren't cleaned away

      code_files.each do |file|
        next unless File.file?(file)
        begin
          content = File.read(file)
          extract_urls(content, urls)
        rescue
          puts "  Warning: Could not read #{file}"
        end
      end

      # Process URLs to extract path components
      urls.each do |url|
        words.merge(extract_words_from_url(url))
      end
    end

    @results[:repos][repo_name] = {
      name: repo_name,
      description: @descriptions[repo_name],
      urls: urls.to_a.sort,
      words: words.to_a.sort
    }
  end

  def extract_urls(content, urls)
    content.scan(/https?:\/\/[^\s<>"']+/).each do |url|
      begin
        uri = URI.parse(url)
        urls << url if uri.host
      rescue URI::InvalidURIError
        # Skip invalid URLs
      end
    end
  end

  def extract_words_from_url(url)
    begin
      uri = URI.parse(url)
      # Get path and query parameters
      path = uri.path
      query = uri.query
      return [] unless path && !path.empty? && path != '/'

      # Get path components
      parts = path.downcase.split(/[^a-z0-9]+/).reject(&:empty?)

      # Add query parameter names and values
      if query
        query.split('&').each do |param|
          name, value = param.split('=', 2)
          parts << name.downcase if name
          parts << value.downcase if value && !value.empty?
        end
      end

      # Filter parts that are too short or just numbers
      parts = parts.select { |part| part.length > 2 && part =~ /[a-z]/ }

      # Filter out dictionary words
      filter_dictionary_words(parts)
    rescue URI::InvalidURIError
      []
    end
  end

  def filter_dictionary_words(terms)
    return [] if terms.empty?

    # Write terms to temp file
    temp_file = 'temp_words.txt'
    File.write(temp_file, terms.join("\n"))

    # Run aspell to get non-dictionary words
    output, _ = Open3.capture2("#{SPELL_CMD} < #{temp_file}")

    # Clean up
    FileUtils.rm_f(temp_file)

    output.split("\n")
  end

  def output_results
    # Production JS file - minimal data needed for frontend
    js_content = <<~JS
      // Generated by Plannies Mate at #{@results[:metadata][:generated_at]}
      // Analyzed #{@results[:metadata][:repos_analyzed]} repositories

      export const scraperData = {
      #{@results[:repos].map { |name, data|
      "  '#{name}': {\n" \
        "    description: #{data[:description].to_json},\n" \
        "    words: #{data[:words].inspect}\n" \
        "  }"
    }.join(",\n")}
      };
    JS

    FileUtils.mkdir_p('tmp')
    File.write('tmp/scraper_analysis.js', js_content)

    # Debug JSON file - full analysis including URLs
    File.write('tmp/debug_analysis.json', JSON.pretty_generate(@results))

    puts "\n# Analysis Results"
    puts "Generated tmp/scraper_analysis.js with search terms and descriptions"
    puts "Generated tmp/debug_analysis.json with full analysis including URLs"
    puts "Analyzed #{@results[:metadata][:repos_analyzed]} repositories"
    puts "Generated at: #{@results[:metadata][:generated_at]}"
  end
end

if __FILE__ == $0
  if ARGV.empty?
    abort "Usage: #{$0} REPO_DIR"
  end

  analyzer = ScraperAnalyzer.new(ARGV[0])
  analyzer.analyze
end
